# -*- coding: utf-8 -*-
"""FRB_loglike.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fMaa1fiqKD_NZiPGDV2c7a54QphyY6hg
"""

import jax.numpy as jnp
from jax import lax, vmap, random
#!pip install numpyro
import numpyro
import numpyro.distributions as dist
import astropy as ap
import astropy.units as apu
import matplotlib.pyplot as plt
#!pip install corner
import corner
import numpy as np


from jax.scipy.stats import gaussian_kde
from jax.config import config
#config.update('jax_debug_nans',True)


"""# **Forward and statistical model**"""
# Cycle over FRBs
def do_like(mu, sig2, sigmah_mat):
    # Integrate over sigmah.
    like = integ_sigmah(mu,sig2,sigmah_mat)
    return like
v_do_like = vmap(do_like, in_axes=(0,0,0), out_axes=(0))

# Integrate over sigmah
def integ_sigmah(mu,sig2,sigmah_arr):
    # a is a 1D array here, with each element corresponding to a sigmah value from the prior divided by 1/1+z
    e = -0.5 * mu**2/(sig2 + sigmah_arr**2) - 0.5*jnp.log(2 * jnp.pi * (sig2 + sigmah_arr**2))
    a = jnp.exp(e) * p_sigmah
    # Integrate over sigmah (NOT over sigmah/1+z)
    I = jnp.trapz(a,sigmah_vals,axis=0)
    return I

# Divide sigmah values by 1+z
def div_1_z(sigmah_arr,z):
    sigmah_arr = sigmah_arr.at[:].divide(1+z)
    return sigmah_arr

    
class mylike(dist.Distribution):
    support = dist.constraints.real

    def __init__(self, mu, sig2, z, fp): 
        self.mu, self.sig2, self.z, self.fp = dist.util.promote_shapes(mu, sig2, z, fp)
        batch_shape = lax.broadcast_shapes(jnp.shape(mu), jnp.shape(sig2), jnp.shape(z), jnp.shape(fp))
        super(mylike, self).__init__(batch_shape = batch_shape)
        
    def sample(self, key, sample_shape=()):
        raise NotImplementedError
        
    def log_prob(self, value):
        mu = self.mu
        sig2 = self.sig2
        z = self.z
        fp = self.fp        

        #like = -0.5*(value - mu)**2/sig2 - 0.5*jnp.log(2*jnp.pi*sig2)
        #print(f'loglike = {like + fp}') 
        # Divide sigmah values by 1+z values
        #sigmah_mat_1_z = vmap(div_1_z, in_axes=(0,0))(sigmah_mat,z)

        # Marginalise over the first stage sigmah posterior
        #like = v_do_like(mu-value, sig2, sigmah_mat_1_z)
        #print(f'like = {like}')
        
        like = -0.5*(value - mu)**2/sig2 - 0.5*jnp.log(2*jnp.pi*sig2)

        return like + fp


def bkgrnd_prefactor():
    # If we use the BORG fields, we need a fixed H0
    prefactor = 3 * H0 * ap.constants.c  / (8 * jnp.pi * ap.constants.G * ap.constants.m_p)
    prefactor = prefactor.to(apu.pc / apu.cm**3)
    prefactor = prefactor.value
    return prefactor

def bkgrnd_integrand(x):
    return (1 + x)/ jnp.sqrt(Om*(1+x)**3 + 1 - Om)

def bkgrnd_integral(y):
    x = jnp.linspace(0,y,1000)
    integrand = bkgrnd_integrand(x)
    return jnp.trapz(integrand, x, axis=0)
    
    
def fluct_prefactor():
    prefactor = 3 * H0**2 / (8 * jnp.pi * ap.constants.G * ap.constants.m_p)
    prefactor = prefactor.to(1 / apu.cm**3)
    prefactor = prefactor.value
    return prefactor
    
    
def get_interp(z, tabulated_vals):
    return jnp.interp(z,tabulated_z,tabulated_vals)
v_get_interp = vmap(get_interp, in_axes=(0,0), out_axes=(0))
 

def fluct_integral(z, i, tabulated_integral, tabulated_z):
    # Compare the drawn z values to the tabulated ones, and extract the precalculated integral values here
    print(f'i = {i}')
    print(f'shape, tabulated_integral[i,:,:] = {jnp.shape(tabulated_integral[i,:,:])}')
    return v_get_interp(z, tabulated_integral[i,:,:])
    

def uncon_integral(z, tabulated_ccl, tabulated_z):
    return v_get_interp(z, tabulated_ccl)

def make_mock(ztrue, obchif_true, muh_true, sigmah_true, obchif_bias_true, dm_err):

    nfrb = len(ztrue)    
    dm_los = bkgrnd_prefactor() * obchif_true * bkgrnd_integral(ztrue)
    f_prefactor = fluct_prefactor() * obchif_bias_true
    dm_fluct = f_prefactor * fluct_integral(ztrue, borg_index, tabulated_integral, tabulated_z) * 1.e6
    # TEST
    print(f'fluct_prefactor = {fluct_prefactor()}')

    print(f'ztrue = ')
    print(ztrue)
    print(f'dm_los = ')
    print(dm_los)
    print(f'dm_fluct = ')
    print(dm_fluct)
    dm_los = random.normal(random.PRNGKey(1), (nfrb,)) * dm_err + dm_los + dm_fluct
    
    dm_host = random.normal(random.PRNGKey(2), (nfrb,)) * sigmah_true + muh_true
    # 1/(1+z) factor
    dm_host = dm_host.at[:].divide(1+ztrue[:])
    
    sig_uncon = jnp.sqrt(uncon_integral(ztrue, tabulated_ccl, tabulated_z)) * f_prefactor * 1.e6
    dm_uncon = random.normal(random.PRNGKey(3), (nfrb,)) * sig_uncon
    
    dm_tot = dm_los + dm_host + dm_uncon
       
    return dm_tot
   
def get_loglike(obchif,muh,sigmah,obchif_bias,z):
    # Construct quantities for likelihood.                                                                           
    positions = jnp.zeros((4))
    positions = positions.at[0].set(obchif)
    positions = positions.at[1].set(muh)
    positions = positions.at[2].set(sigmah)
    positions = positions.at[3].set(obchif_bias)
                                                 
    first_stage_prior = get_first_stage_prior_val(positions)
    #fsp = jnp.ones((nfrb)) * first_stage_prior
    fsp = jnp.zeros((nfrb))


    # Mean:
    # Background: array of length nfrb
    dm_b = bkgrnd_prefactor() * bkgrnd_integral(z) * obchif
    # Fluctuation: array of length nfrb
    dm_f = fluct_prefactor() * obchif_bias * fluct_integral(z, borg_index, tabulated_integral, tabulated_z) * 1.e6
    # Host: array of length nfrb
    dm_h = muh/(1+z)
    
    dm_pred = dm_b + dm_f + dm_h
                                                                                                                   
    # Variance
    # Unconstrained: array of length nfrb
    sig_uncon2 = (fluct_prefactor() * obchif_bias * 1.e6 * littleh * 2) ** 2 * uncon_integral(z, tabulated_ccl, tabulated_z)
    # obchif: array of length nfrb
    #sig_obchif2_arr = (bkgrnd_prefactor() * bkgrnd_integral(z))**2 *sig_obchif2
    # muh: array of length nfrb
    #sig_muh2_arr = sig_muh2/(1+z)**2
    sig_h2 = (sigmah/(1+z))**2 
    dm_sig2 = dm_err**2 + dm_mw_err**2 + sig_uncon2 + sig_h2 #+ sig_obchif2_arr + sig_muh2_arr

    ll = mylike(dm_pred,dm_sig2,z,fsp)
    loglike = ll.log_prob(dm_meas)
    print(f'loglike = {loglike}')
    return jnp.sum(loglike) 

    
def closest_arg(ymeas, yall):
    return jnp.argmin(jnp.abs(yall - ymeas))
v_closest_arg = vmap(closest_arg, in_axes=(0, None), out_axes=(0))

def divide_1_z(arr,z_arr):
    div_arr = jnp.zeros((len(arr)))
    div_arr = arr.at[:].divide(1+z_arr)
    return div_arr

def get_obchif_bias_first_stage_prior_val(param_vec):
    # Approximate the posterior of the first stage as a gaussian mixture model.

    pref_1 = 1/(jnp.sqrt(2*jnp.pi)**2 * jnp.sqrt(det_cov_1))
    pref_2 = 1/(jnp.sqrt(2*jnp.pi)**2 * jnp.sqrt(det_cov_2))
   
    #print(f'prefactors: pref_1 = {pref_1}, pref_2 = {pref_2}')

 
    mu_pos = param_vec - means[0]
    mu_neg = param_vec - means[1]

    m1_pos = jnp.matmul(inv_cov_1,mu_pos.reshape(-1,1))
    m2_pos = jnp.matmul(mu_pos,m1_pos)

    
    m1_neg = jnp.matmul(inv_cov_2,mu_neg.reshape(-1,1))
    m2_neg = jnp.matmul(mu_neg,m1_neg)

    prior_val =  weights[1] * pref_1 * jnp.exp(-0.5 * m2_pos) + weights[0] * pref_2 * jnp.exp(-0.5 * m2_neg)


    return jnp.log(prior_val)


def model():
    # Sample parameters from prior
    sigmah = numpyro.sample("sigmah", dist.Uniform(sigmah_min, sigmah_max))
    obchif_bias = numpyro.sample("obchif_bias", dist.Uniform(obchif_bias_min, obchif_bias_max))

    # Modulate with first stage prior
    positions = jnp.zeros((2))                
    positions = positions.at[0].set(sigmah)
    positions = positions.at[1].set(obchif_bias)
 
    first_stage_prior = get_obchif_bias_first_stage_prior_val(positions)
    fsp = jnp.ones((nfrb)) * first_stage_prior/nfrb

    # Sample redshift
    if nfrb > nlocal:
        z_unknown = numpyro.sample("z", dist.Uniform(z_min, z_max), sample_shape=(nfrb - nlocal,))
        z = jnp.concatenate([z_known,z_unknown])
    else:
        z = z_known
    
    # Construct quantities for likelihood.
    # Mean:
    # Background: array of length nfrb
    dm_b = bkgrnd_prefactor() * bkgrnd_integral(z) * mu_obchif
    # Fluctuation: array of length nfrb
    dm_f = fluct_prefactor() * obchif_bias * fluct_integral(z, borg_index, tabulated_integral, tabulated_z) * 1.e6
    # Host: array of length nfrb
    dm_h = mu_muh/(1+z)
    
    dm_pred = dm_b + dm_f + dm_h

    # Variance
    # Unconstrained: array of length nfrb
    sig_uncon2 = (fluct_prefactor() * obchif_bias * 1.e6 * littleh * 2) ** 2 * uncon_integral(z, tabulated_ccl, tabulated_z)
    # host:
    sig_h2 = (sigmah/(1+z))**2
    # obchif:
    sig_obchif2 = (bkgrnd_prefactor() * bkgrnd_integral(z))**2 * var_obchif
    # muh:
    sig_muh2 = var_muh / (1+z)**2
    # covariance 
    sig_covar = cov_obchif_muh * (bkgrnd_prefactor() * bkgrnd_integral(z)) / (1+z)
 
    dm_sig2 = dm_err**2 + dm_mw_err**2 + sig_uncon2 + sig_h2 + sig_obchif2 + sig_muh2 + 2 * sig_covar

    numpyro.sample("obs", mylike(dm_pred, dm_sig2, z, fsp), obs=dm_meas)


##############
"""# **Set up inference parameters**"""

# PRIORS
obchif_min, obchif_max = 0, 0.1
obchif_bias_min, obchif_bias_max = -0.5, 0.5
muh_min, muh_max = 0, 500
sigmah_min, sigmah_max = 0, 400
z_min, z_max = 0, 4.0

# TRUTHS
H0 = 67.74 * apu.km / apu.s / apu.Mpc
littleh = 0.6774
Om = 0.3089
obchif_true = 0.04 #0.3
obchif_bias_true = 0.04 * 1
host_mean = 500
host_std = 300
muh_true = 150 #jnp.log(host_mean ** 2 / jnp.sqrt(host_mean ** 2 + host_std ** 2))
sigmah_true = 20 #jnp.log(1 + host_std ** 2 / host_mean ** 2)
dm_err = 5


# HYPERPARAMETERS
delta_t = 10            # Spacing of t grid
delta_z = 0.01          #Â Spacing for initial z guess
tmin = 0                # Min value for convolution
tmax = 3.e4             # Max value for convolution
num_samples = 20000
num_warmup = 2000
borg_index = 15
nlocal = 0
nunknown = 100
imin = 50              # where to start mock (not important is shuffle_mock=True)
shuffle_mock = False

# DIRECTORIES
mock_path = './'
fig_dir = './'
data_path = './'

print(f'true parameters: obchif = {obchif_true}, muh = {muh_true}, sigmah = {sigmah_true}, obchif_bias = {obchif_bias_true}')

print(f'background scaling = {bkgrnd_prefactor() * obchif_true}')
print(f'Fluct scaling = {fluct_prefactor() * obchif_bias_true * 1.e6}')
"""# **Second stage inference- infer all parameters from unlocalised FRBs**"""

# Load unlocalised frbs
hf = jnp.load(data_path+'/sdss_frbs_ne2001.npz')
data = hf['data']

repeats = [24,25,44,47,48,60,61,66,67,68,69,70,73,102,124,127]#,4,119,130,56,87,107,138]
idx = np.arange(156)
jdx = np.ones((156),dtype=int)
for i in range(156):
    for j in range(len(repeats)):
        if idx[i] == repeats[j]:
            jdx[i] = 0
# Load data but remove catalogue repeats
dm_meas = data[jdx==1,2]
#ztrue = jnp.asarray([])
ztrue = jnp.empty((len(dm_meas)))
z_known = jnp.asarray([])
nfrb = len(dm_meas)
nlocal = len(z_known)

# Load integrals
data = jnp.load(mock_path + 'tabulated_integrals_sdss_6000_9000.npz')
tabulated_z = jnp.array(data['tabulated_z'])
tabulated_integral = jnp.array(data['tabulated_integrals'])[:,jdx==1,:]
tabulated_exceeds = jnp.array(data['tabulated_exceeds'])[jdx==1,:]
tabulated_ccl = jnp.array(data['tabulated_ccl'])[jdx==1,:]

print(f'Shape, tabulated integral = {jnp.shape(tabulated_integral)}')
print(f'borg index = {borg_index}')


# Load the gaussian mixture model fit to the first stage
# Here, we have one fit to (obchif,muh) with a one-component model
# and a second fit to obchif_bias with a two-component model

file = 'gm_fit_test.npz'
d = np.load(file)
weights = d['weights']
means = d['means']
covs = d['covs']
print(f'prior weights = {weights}')

#sig1 = covs[0,0,0]
#sig1 = covs[1,0,0]
det_cov_1 = np.linalg.det(covs[0])
det_cov_2 = np.linalg.det(covs[1])
inv_cov_1 = np.linalg.inv(covs[0])
inv_cov_2 = np.linalg.inv(covs[1])
dm_mw_err = 30

# Load first stage constraints
file = 'first_stage_constraints.npz'
d = np.load(file)
mu_obchif = np.mean(d['obchif_samples'])
var_obchif = np.var(d['obchif_samples'])
mu_muh = np.mean(d['muh_samples'])
var_muh = np.var(d['muh_samples'])
obchif_muh = np.zeros((2,len(d['obchif_samples'])))
obchif_muh[0,:] = d['obchif_samples']
obchif_muh[1,:] = d['muh_samples']
covar = np.cov(obchif_muh)
cov_obchif_muh = covar[0,1]
print(f'Var, obchif = {var_obchif}, cov est = {covar[0,0]}')
print(f'Var, muh = {var_muh}, cov est = {covar[1,1]}')
print(f'Covar, obchif-muh = {cov_obchif_muh}')

sigmah_samps =d['muh_samples']

nbins = 25
p_sigmah, edges = jnp.histogram(sigmah_samps,bins=nbins,density=True)
sigmah_vals = jnp.zeros((nbins))
sigmah_mat = jnp.zeros((nfrb,nbins))
for i in range(nbins):
    sigmah_vals = sigmah_vals.at[i].set( (edges[i+1] + edges[i])/2)
for i in range(nfrb):
    sigmah_mat = sigmah_mat.at[i,:].set(sigmah_vals)




print('NFRB', nfrb)
print('NLOCAL', nlocal)
print('dm_meas', dm_meas)


"""# **Initialise and run**"""
##  Run sampler

# Initialise to random position in prior:
val = 5
rng_key = random.PRNGKey(val)
rng_key, rng_key_ = random.split(rng_key)
#kernel = numpyro.infer.NUTS(model,step_size=1,adapt_step_size=False) # Default step_size = 1, adapt_step_size = True
#print(f'Init to prior, prngkey = {val}')

#val = 'neg'
#values={'muh':muhs,'sigmah':sigs,'obchif':obs,'obchif_bias':obbs,'z':zs}
#kernel = numpyro.infer.NUTS(model,init_strategy=numpyro.infer.initialization.init_to_value(values=values))
#print(f'Init to negative bias')

# Initialise to median of prior
val = 'med'
kernel = numpyro.infer.NUTS(model, init_strategy=numpyro.infer.initialization.init_to_median(num_samples=250))
# Play with some things
#kernel = numpyro.infer.NUTS(model, init_strategy=numpyro.infer.initialization.init_to_median(num_samples=250), dense_mass=True, max_tree_depth=20)
print(f'Init to median')



mcmc = numpyro.infer.MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples)


mcmc.warmup(rng_key,collect_warmup=True)
warmup_samples = mcmc.get_samples()
mcmc.run(rng_key_)

mcmc.print_summary()

"""# **Extract Samples**"""

# Convert samples into a single array
samples = mcmc.get_samples()
nparam = len(samples.keys())
if nfrb > nlocal:
    nparam += nfrb - nlocal - 1
samps = jnp.empty((num_samples, nparam))
param_names = ['sigmah','obchif_bias'] #['global_params','obchif_bias']
param_labels = [r'$\sigmah_{\rm h}$',r'$\Omega_{\rm b} \chi f b$'] #['global_params',r'$\Omega_{\rm b} \chi f b$']##, 

# Save samples
np.savez(f'second_stage_constraints_{borg_index}_{val}',sigmah_samples=samples['sigmah'],obchif_bias_samples=samples['obchif_bias'],z_samples=samples['z'])
#np.savez('mock_first_stage_constraints',obchif_samples=samples['obchif'],muh_samples=samples['muh'],sigmah_samples=samples['sigmah'])

#ll = get_loglike(jnp.mean(samples['obchif']),jnp.mean(samples['muh']),jnp.mean(samples['sigmah']),jnp.mean(samples['obchif_bias']),jnp.mean(samples['z'],axis=0))
#print(f'loglike = {ll}')



"""
for i, p in enumerate(param_names):
    samps = samps.at[:,i].set(samples[p])
if nfrb > nlocal:
    samps = samps.at[:,len(param_names):].set(samples['z'])
    param_names = param_names + ['z%i'%i for i in range(nfrb-nlocal)]
    param_labels = param_labels + [r'$z_{%i}$'%i for i in range(nfrb-nlocal)]
samps = np.asarray(samps)

#truths = list([obchif_true, obchif_bias_true, muh_true, sigmah_true]) + list(ztrue[nlocal:])
#truths = [float(f) for f in truths]
"""
"""## **Plotting Scripts**"""
"""
#Â Trace plot of non-redshift quantities
fig1, axs1 = plt.subplots(4, 1, figsize=(6,7), sharex=True)
for i in range(4):
    axs1[i].plot(samps[:,i])
    axs1[i].set_ylabel(param_labels[i])
    axs1[i].axhline(truths[i], color='k')
axs1[-1].set_xlabel('Step Number')
fig1.tight_layout()
fig1.savefig(fig_dir + f'/second_stage_trace_{borg_index}_{val}.png')
"""

"""
# Corner plot
fig2, axs2 = plt.subplots(nparam-(nfrb-nlocal), nparam-(nfrb-nlocal), figsize=(8,8))
if nfrb > nlocal:
    corner.corner(
        samps[:,:-(nfrb-nlocal)],
        labels=param_labels[:-(nfrb-nlocal)],
        fig=fig2,
        truths=truths[:-(nfrb-nlocal)]
    )
else:
    corner.corner(
        samps,
        labels=param_labels,
        fig=fig2,
        truths=truths
    )
fig2.savefig(fig_dir + '/corner.png')

# Redshift residuals plot
if nfrb > nlocal:
    fig3, axs3 = plt.subplots(1, 1, figsize=(8,4))
    median = np.median(samps, axis=0)
    sigma_plus = np.percentile(samps, 84, axis=0) - median
    sigma_minus = median - np.percentile(samps, 16, axis=0)
    median = median[-(nfrb-nlocal):]
    sigma_plus = sigma_plus[-(nfrb-nlocal):]
    sigma_minus = sigma_minus[-(nfrb-nlocal):]
    x = ztrue[nlocal:]
    y = median - x
    plot_kwargs = {'fmt':'.', 'markersize':3, 'zorder':-1,
                 'capsize':1, 'elinewidth':1, 'color':'k', 'alpha':1}
    axs3.errorbar(x, y, yerr=[sigma_minus, sigma_plus], **plot_kwargs)
    axs3.axhline(0, color='k')
    axs3.set_xlabel(r'$z_{\rm true}$')
    axs3.set_xlabel(r'$z_{\rm infer} - z_{\rm true}$')
    fig3.tight_layout()
    fig3.savefig(fig_dir + '/inferred.png')

plt.show()
"""
"""
plt.figure()
x = np.linspace(0,300,100)
norm = 1/(jnp.sqrt(2*jnp.pi)*120) * jnp.exp(-0.5 * (160-x)**2/120**2)
lnorm = 1/(jnp.sqrt(2*jnp.pi)*0.7*x) * jnp.exp(-0.5 * (4.95-jnp.log(x))**2/0.7**2)
plt.plot(x,norm,'b',label='normal')
plt.plot(x,lnorm,'r',label='lognormal')
plt.legend()
plt.xlabel('DM_host')
plt.ylabel('p(DM_host)')

ln_obchif_samp = samples['obchif']
n_obchif_samp = np.load('first_stage_constraints_20.npz')['obchif_samples']
plt.figure()
plt.hist(ln_obchif_samp,density=True,color='r',alpha=0.7, histtype='step',label='lognormal',bins=50)
plt.hist(n_obchif_samp,density=True,color='b',alpha=0.7, histtype='step', label='normal',bins=50)
plt.xlabel('obchif')
plt.ylabel('p(obchif)')
plt.legend()

# Construct informative prior based on first inference step -- Mock data
obchifs = samples['obchif']
muhs = samples['muh']
sigmahs = samples['sigmah']
Prior_Means = jnp.asarray([jnp.mean(obchifs),jnp.mean(muhs),jnp.mean(sigmahs)])
smatrix = jnp.stack((obchifs,muhs,sigmahs), axis=0)
Prior_Cov = jnp.cov(smatrix)
"""
"""# **Second Stage Plots**"""

flat_samps = np.zeros((num_samples,5))
#flat_samps[:,0] = samples['obchif']
#flat_samps[:,1] = samples['muh']
flat_samps[:,0] = samples['sigmah']
flat_samps[:,1] = samples['obchif_bias']
flat_samps[:,2] = samples['z'][:,0]
flat_samps[:,3] = samples['z'][:,1]
flat_samps[:,4] = samples['z'][:,2]


labels = ['sigmah','obchif_bias','z0','z1','z2']
#truths = [obchif_bias_true]

fig = corner.corner(
        flat_samps,
        labels=labels,
    )
plt.savefig(f'second_stage_corner_{borg_index}_{val}.png')

full_flat_samps = np.zeros((num_warmup+num_samples,5))
#full_flat_samps[:,0] = np.concatenate((warmup_samples['obchif'],samples['obchif']))
#full_flat_samps[:,1] = np.concatenate((warmup_samples['muh'],samples['muh']))
full_flat_samps[:,0] = np.concatenate((warmup_samples['sigmah'],samples['sigmah']))
full_flat_samps[:,1] = np.concatenate((warmup_samples['obchif_bias'],samples['obchif_bias']))
full_flat_samps[:,2] = np.concatenate((warmup_samples['z'][:,0],samples['z'][:,0])) 
full_flat_samps[:,3] = np.concatenate((warmup_samples['z'][:,1],samples['z'][:,1]))  
full_flat_samps[:,4] = np.concatenate((warmup_samples['z'][:,2],samples['z'][:,2])) 


fig1, axs1 = plt.subplots(5, 1, figsize=(6,7), sharex=True)
for i in range(5):
    axs1[i].plot(full_flat_samps[:,i])
    axs1[i].set_ylabel(labels[i])
    #axs1[i].axhline(truths[i], color='k')
    vmin = np.amin(full_flat_samps[:,i])
    vmax = np.amax(full_flat_samps[:,i])
    vline = np.linspace(vmin,vmax,100)
    vval = np.ones((100)) * num_warmup 
    axs1[i].plot(vval,vline,'k--')
axs1[-1].set_xlabel('Step Number')
fig1.tight_layout()
fig1.savefig(fig_dir + f'/second_stage_trace_{borg_index}_{val}.png')





"""
zsamps = np.zeros((num_samples,nunknown))
for i in range(nfrb-nlocal):
    zsamps[:,i] = samples['z'][:,i]

if nunknown > 2:
    labels = ['z0','z1','z2']
    truths = [ztrue[nlocal],ztrue[nlocal+1],ztrue[nlocal+2]]
    fig = corner.corner(
        zsamps[:,:3],
        labels=labels,
        truths=truths
        )
    plt.savefig('second_stage_zcorner.png')


fig3, axs3 = plt.subplots(1, 1, figsize=(8,4))
median = np.median(zsamps, axis=0)
sigma_plus = np.percentile(zsamps, 84, axis=0) - median
sigma_minus = median - np.percentile(zsamps, 16, axis=0)
median = median[nlocal:]
sigma_plus = sigma_plus[nlocal:]
sigma_minus = sigma_minus[nlocal:]
x = ztrue[nlocal:]
y = median - x
plot_kwargs = {'fmt':'.', 'markersize':3, 'zorder':-1,
              'capsize':1, 'elinewidth':1, 'color':'k', 'alpha':1}
axs3.errorbar(x, y, yerr=[sigma_minus, sigma_plus], **plot_kwargs)
axs3.axhline(0, color='k')
axs3.set_xlabel(r'$z_{\rm true}$')
axs3.set_xlabel(r'$z_{\rm infer} - z_{\rm true}$')
fig3.tight_layout()
fig3.savefig(fig_dir + '/second_stage_z_inferred.png')
"""
